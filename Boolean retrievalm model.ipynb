{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEDjpIefAaVi",
        "outputId": "1464427a-45c2-4c32-b520-789828e2f576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining directory**"
      ],
      "metadata": {
        "id": "tZw2ZMeCCaCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"ResearchPapers\"\n",
        "tokens = {}\n",
        "No_stop_word_tokens = {}\n"
      ],
      "metadata": {
        "id": "qg3vhu8UA6X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functions to process tokens**"
      ],
      "metadata": {
        "id": "i0XWGcA6CkzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(tokens):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    # Iterate through the tokens and filter out URLs\n",
        "    tokens_without_urls = [token for token in tokens if not url_pattern.match(token)]\n",
        "    return tokens_without_urls\n"
      ],
      "metadata": {
        "id": "ON9ctTMDGtOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(tokens):\n",
        "    # Define the pattern to match special characters\n",
        "    special_char_pattern = re.compile(r'[^\\w\\s]')  # Matches any character that is not a word character or whitespace\n",
        "\n",
        "    # Iterate through the tokens and remove special characters\n",
        "    tokens_without_special_chars = [special_char_pattern.sub('', token) for token in tokens]\n",
        "\n",
        "    # Remove empty tokens after removing special characters\n",
        "    tokens_without_special_chars = [token for token in tokens_without_special_chars if token]\n",
        "\n",
        "    return tokens_without_special_chars"
      ],
      "metadata": {
        "id": "B7LMythNGxtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(tokens):\n",
        "    # Define the pattern to match numbers\n",
        "    number_pattern = re.compile(r'\\b\\d+\\b')\n",
        "\n",
        "    # Iterate through the tokens and remove tokens containing numbers\n",
        "    tokens_without_numbers = [token for token in tokens if not number_pattern.match(token)]\n",
        "\n",
        "    return tokens_without_numbers\n"
      ],
      "metadata": {
        "id": "vBHlVyFoHlOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting data from file , performing tokenization and then process those tokens**"
      ],
      "metadata": {
        "id": "CJpkt1P7CwWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_data = {}\n",
        "n = 0\n",
        "for filename in os.listdir(directory):\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        file_content = file.read()\n",
        "        tokens = word_tokenize(file_content)\n",
        "        ps = PorterStemmer()\n",
        "        token_case_fold = [ps.stem(word) for word in tokens]\n",
        "\n",
        "        custom_stop_words = set([\n",
        "            'a', 'is', 'the', 'of', 'all', 'and', 'to', 'can', 'be', 'as',\n",
        "            'once', 'for', 'at', 'am', 'are', 'has', 'have', 'had', 'up',\n",
        "            'his', 'her', 'in', 'on', 'no', 'we', 'do'\n",
        "        ])\n",
        "\n",
        "        punctuation = set(string.punctuation)\n",
        "\n",
        "        # Removing punctuation and stop words\n",
        "        No_punctuation_tokens = [word for word in token_case_fold if word not in punctuation]\n",
        "        No_stop_word_tokens = [word for word in No_punctuation_tokens if word.lower() not in custom_stop_words]\n",
        "\n",
        "        No_url_tokens= remove_urls(No_stop_word_tokens)\n",
        "\n",
        "        No_special_char_tokens = remove_special_characters(No_url_tokens)\n",
        "\n",
        "        No_number_tokens = remove_numbers(No_special_char_tokens)\n",
        "\n",
        "\n",
        "        # Store document data\n",
        "        document_data[filename] = {\n",
        "            'tokens': No_number_tokens,  # Store processed tokens\n",
        "             'term_count': len(set(No_number_tokens))\n",
        "        }\n",
        "\n",
        "#printing tokens:\n",
        "# for filename, data in document_data.items():\n",
        "#     print(f\"Document: {filename}\")\n",
        "#     print(\"Processed Tokens:\", data['tokens'])\n",
        "#     print()\n",
        "\n",
        "grand_total = sum(data['term_count'] for data in document_data.values())\n",
        "print(\"Grand Total of Terms:\", grand_total)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhnGWdbaBBGa",
        "outputId": "427b954b-506c-4793-8150-bde33c29c2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grand Total of Terms: 39466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Inverted index**\n"
      ],
      "metadata": {
        "id": "G1TgE9-CgkIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, data=None):\n",
        "        self.data = data\n",
        "        self.next = None\n",
        "\n",
        "\n",
        "class LinkedList:\n",
        "    def __init__(self):\n",
        "        self.head = None\n",
        "\n",
        "    def append(self, data):\n",
        "        new_node = Node(data)\n",
        "        if not self.head:\n",
        "            self.head = new_node\n",
        "            return\n",
        "        current = self.head\n",
        "        while current.next:\n",
        "            current = current.next\n",
        "        current.next = new_node\n",
        "\n",
        "    def __str__(self):\n",
        "        result = []\n",
        "        current = self.head\n",
        "        while current:\n",
        "            result.append(str(current.data))\n",
        "            current = current.next\n",
        "        return ' -> '.join(result)\n",
        "\n",
        "\n",
        "# Implement inverted index\n",
        "inverted_index = {}\n",
        "document_frequency = {}\n",
        "\n",
        "for filename, data in document_data.items():\n",
        "    tokens = data['tokens']\n",
        "    unique_tokens_in_doc = set(tokens)\n",
        "    for token in unique_tokens_in_doc:\n",
        "        if token not in inverted_index:\n",
        "            inverted_index[token] = LinkedList()\n",
        "            document_frequency[token] = 1\n",
        "        else:\n",
        "            document_frequency[token] += 1\n",
        "        inverted_index[token].append(filename)\n",
        "\n",
        "# Printing the inverted index:\n",
        "# for term, posting_list in inverted_index.items():\n",
        "#     print(f\"Term: {term}, Posting List: {posting_list}, Document Frequency: {document_frequency[term]}\")\n",
        "\n",
        "\n",
        "# this is the terms we search present in the query\n",
        "search_terms = ['learn' , 'model' , 'cancer']\n",
        "\n"
      ],
      "metadata": {
        "id": "FkXkceEQELAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intersection**"
      ],
      "metadata": {
        "id": "cnheDXnZGYch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents_containing_terms = set(document_data.keys())  # Initialize with all documents\n",
        "#intersection\n",
        "for term in search_terms:\n",
        "    if term in inverted_index:\n",
        "        posting_list = inverted_index[term]\n",
        "        current = posting_list.head\n",
        "        documents_containing_this_term = set()\n",
        "\n",
        "        # Add documents containing the current term to a temporary set\n",
        "        while current:\n",
        "            documents_containing_this_term.add(current.data)\n",
        "            current = current.next\n",
        "\n",
        "        # Intersect with the current set of documents\n",
        "        documents_containing_terms.intersection_update(documents_containing_this_term)\n",
        "\n",
        "\n",
        "print(f\"Documents containing {search_terms} are {documents_containing_terms} \")\n"
      ],
      "metadata": {
        "id": "LO5GJKLrF5lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Union**"
      ],
      "metadata": {
        "id": "-yjHsjsmGdYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Union operation\n",
        "documents_containing_terms = set()  # Initialize an empty set\n",
        "\n",
        "for term in search_terms:\n",
        "    if term in inverted_index:\n",
        "        posting_list = inverted_index[term]\n",
        "        current = posting_list.head\n",
        "\n",
        "        # Add documents containing the current term to the set\n",
        "        while current:\n",
        "            documents_containing_terms.add(current.data)\n",
        "            current = current.next\n",
        "\n",
        "for doc_id in documents_containing_terms:\n",
        "    print(f\"Document ID: {doc_id}\")\n"
      ],
      "metadata": {
        "id": "bYSmL3PPGDLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOT OPERATION**"
      ],
      "metadata": {
        "id": "kkQ5O02TGjth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOT operation\n",
        "# Initialize a set with all document IDs\n",
        "all_documents = set(document_data.keys())\n",
        "\n",
        "# Initialize an empty set for documents containing the search terms\n",
        "documents_containing_terms = set()\n",
        "\n",
        "# Add documents containing the search terms to the set\n",
        "for term in search_terms:\n",
        "    if term in inverted_index:\n",
        "        posting_list = inverted_index[term]\n",
        "        current = posting_list.head\n",
        "\n",
        "        # Add documents containing the current term to the set\n",
        "        while current:\n",
        "            documents_containing_terms.add(current.data)\n",
        "            current = current.next\n",
        "\n",
        "# Perform the NOT operation to exclude documents containing the search terms\n",
        "documents_not_containing_terms = all_documents - documents_containing_terms\n",
        "\n",
        "# Print the documents not containing the search terms\n",
        "for doc_id in documents_not_containing_terms:\n",
        "    print(f\"Document ID: {doc_id}\")\n"
      ],
      "metadata": {
        "id": "WGC_3vaGGHvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positional Index**"
      ],
      "metadata": {
        "id": "9MHazURjg4Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import csv\n",
        "\n",
        "def build_positional_index(docs_dir):\n",
        "    positional_index = defaultdict(lambda: defaultdict(list))\n",
        "    for filename in os.listdir(docs_dir):\n",
        "        if filename.endswith('.txt'):\n",
        "            doc_id = filename[:-4]\n",
        "            doc_path = os.path.join(docs_dir, filename)\n",
        "            tokens = document_data[filename]['tokens']\n",
        "            for position, term in enumerate(tokens):\n",
        "                positional_index[term][doc_id].append(position)\n",
        "    return positional_index\n",
        "\n",
        "def save_positional_index(positional_index, output_file):\n",
        "    with open(output_file, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Term', 'DocID', 'Positions'])\n",
        "        for term, postings in positional_index.items():\n",
        "            for doc_id, positions in postings.items():\n",
        "                positions_str = '[' + ', '.join(map(str, positions)) + ']'\n",
        "                writer.writerow([term, doc_id, positions_str])\n",
        "\n",
        "positional_index = build_positional_index(directory)\n",
        "output_file = \"positional_index.csv\"\n",
        "save_positional_index(positional_index, output_file)\n",
        "\n",
        "# Print the positional index\n",
        "for term, postings in positional_index.items():\n",
        "    print(f\"Term: {term}\")\n",
        "    for doc_id, positions in postings.items():\n",
        "        positions_str = ', '.join(map(str, positions))\n",
        "        print(f\"  Document ID: {doc_id}, Positions: {positions_str}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "INOFd1V6gH4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To process Proximity Query**"
      ],
      "metadata": {
        "id": "cUdyzvW9EB01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def execute_proximity_query(positional_index, term1, term2, distance):\n",
        "    matching_documents = []\n",
        "\n",
        "    if term1 not in positional_index or term2 not in positional_index:\n",
        "        print(\"One or more terms not found in the positional index\")\n",
        "        return matching_documents\n",
        "\n",
        "    # Iterate over documents containing term1\n",
        "    for document in positional_index[term1].keys():\n",
        "        if document in positional_index[term2]:\n",
        "            positions1 = positional_index[term1][document]\n",
        "            positions2 = positional_index[term2][document]\n",
        "\n",
        "            # Check positional proximity\n",
        "            for pos1 in positions1:\n",
        "                for pos2 in positions2:\n",
        "                    if abs(pos1 - pos2) <= distance:\n",
        "                        matching_documents.append(document)\n",
        "                        break  # Break if a match is found within proximity\n",
        "\n",
        "    return matching_documents\n",
        "\n",
        "# Example usage:\n",
        "term1 = \"past\"\n",
        "term2 = \"research\"\n",
        "distance = 3\n",
        "result = execute_proximity_query(positional_index, term1, term2, distance)\n",
        "print(\"Documents containing both\", term1, \"and\", term2, \"with distance\", distance, \"apart:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXnIYwAs334z",
        "outputId": "4109b67b-774e-4815-8f1b-151fba77f437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents containing both past and research with distance 3 apart: ['12', '12']\n"
          ]
        }
      ]
    }
  ]
}